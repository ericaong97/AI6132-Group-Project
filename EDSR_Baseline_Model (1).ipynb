{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "plt.switch_backend('Agg')\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "SCALE_FACTOR = 4\n",
        "BATCH_SIZE = 8\n",
        "LR_SIZE = 96\n",
        "HR_SIZE = LR_SIZE * SCALE_FACTOR\n",
        "TRAIN_SAMPLES = 800\n",
        "EPOCHS = 30\n",
        "PIXEL_LOSS_WEIGHT = 0.001\n",
        "\n",
        "# Set logging level to error to reduce noise\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. KERAS OPTIMIZER AND CALLBACK CONFIGURATION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Using Adam optimizer with a piecewise constant learning rate schedule.\n",
        "\n",
        "optim_edsr = keras.optimizers.Adam(\n",
        "    learning_rate=keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "        boundaries=[5000],\n",
        "        values=[1e-5, 5e-6]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Checkpoint the best model weights based on validation loss\n",
        "best_weights_checkpoint_path = \"best-model.weights.h5\"\n",
        "\n",
        "\n",
        "save_best_cb = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=best_weights_checkpoint_path,\n",
        "    monitor=\"loss\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1.5. QUANTITATIVE METRICS (PSNR and SSIM)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    \"\"\"Peak Signal-to-Noise Ratio (PSNR) calculated over normalized [0, 1] images.\"\"\"\n",
        "    # PSNR is calculated over normalized image data [0, 1]\n",
        "\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    \"\"\"Structural Similarity Index Measure (SSIM) calculated over normalized [0, 1] images.\"\"\"\n",
        "    # SSIM is calculated over normalized image data [0, 1]\n",
        "\n",
        "    return tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. DATA AUGMENTATION FUNCTIONS (TensorFlow Operators)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def flip_left_right(lowres_img, highres_img):\n",
        "    \"\"\"Flips Images to left and right.\"\"\"\n",
        "\n",
        "    # Outputs random values from a uniform distribution in between 0 to 1\n",
        "    rn = tf.random.uniform(shape=(), maxval=1)\n",
        "\n",
        "    # If rn is less than 0.5 it returns original lowres_img and highres_img\n",
        "    # If rn is greater than 0.5 it returns the flipped image\n",
        "    return tf.cond(\n",
        "        rn < 0.5,\n",
        "        lambda: (lowres_img, highres_img),\n",
        "        lambda: (\n",
        "            tf.image.flip_left_right(lowres_img),\n",
        "            tf.image.flip_left_right(highres_img),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def random_rotate(lowres_img, highres_img):\n",
        "    \"\"\"Rotates Images by 90 degrees.\"\"\"\n",
        "\n",
        "    # Outputs random values from uniform distribution in between 0 and 3.\n",
        "    rn = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
        "\n",
        "    # rn signifies number of times the image(s) are rotated by 90 degrees\n",
        "    return tf.image.rot90(lowres_img, rn), tf.image.rot90(highres_img, rn)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. DATASET LOADING (Updated with Augmentation)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def load_or_simulate_dataset(num_samples, batch_size, lr_shape, hr_shape):\n",
        "    \"\"\"\n",
        "    Loads the real DIV2K dataset via TFDS and applies preprocessing and augmentation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Attempting to load real DIV2K dataset via TFDS...\")\n",
        "\n",
        "        train_ds, info = tfds.load(\n",
        "            'div2k/bicubic_x4',\n",
        "            split='train',\n",
        "            as_supervised=True,\n",
        "            with_info=True\n",
        "        )\n",
        "\n",
        "        def filter_min_size(hr_img, lr_img):\n",
        "            hr_shape = tf.shape(hr_img)\n",
        "            hr_h, hr_w = hr_shape[0], hr_shape[1]\n",
        "            return tf.logical_and(hr_h >= HR_SIZE, hr_w >= HR_SIZE)\n",
        "\n",
        "        initial_samples = info.splits['train'].num_examples\n",
        "        train_ds = train_ds.filter(filter_min_size)\n",
        "\n",
        "        print(f\"Initial TFDS samples: {initial_samples}\")\n",
        "\n",
        "        def preprocess_image_pair(hr_img, lr_img):\n",
        "\n",
        "            hr_img = tf.image.convert_image_dtype(hr_img, tf.float32)\n",
        "            lr_img = tf.image.convert_image_dtype(lr_img, tf.float32)\n",
        "\n",
        "\n",
        "            hr_shape = tf.shape(hr_img)\n",
        "            hr_h, hr_w = hr_shape[0], hr_shape[1]\n",
        "\n",
        "            max_offset_h = hr_h - HR_SIZE\n",
        "            max_offset_w = hr_w - HR_SIZE\n",
        "\n",
        "            offset_h = tf.random.uniform(shape=[], minval=0, maxval=max_offset_h + 1, dtype=tf.int32)\n",
        "            offset_w = tf.random.uniform(shape=[], minval=0, maxval=max_offset_w + 1, dtype=tf.int32)\n",
        "\n",
        "            offset_h = (offset_h // SCALE_FACTOR) * SCALE_FACTOR\n",
        "            offset_w = (offset_w // SCALE_FACTOR) * SCALE_FACTOR\n",
        "\n",
        "            hr_patch = tf.image.crop_to_bounding_box(hr_img, offset_h, offset_w, HR_SIZE, HR_SIZE)\n",
        "\n",
        "            lr_offset_h = offset_h // SCALE_FACTOR\n",
        "            lr_offset_w = offset_w // SCALE_FACTOR\n",
        "            lr_patch = tf.image.crop_to_bounding_box(lr_img, lr_offset_h, lr_offset_w, LR_SIZE, LR_SIZE)\n",
        "\n",
        "\n",
        "            lr_patch, hr_patch = flip_left_right(lr_patch, hr_patch)\n",
        "            lr_patch, hr_patch = random_rotate(lr_patch, hr_patch)\n",
        "\n",
        "            return lr_patch, hr_patch\n",
        "\n",
        "        # Apply preprocessing, shuffling, and batching\n",
        "        dataset = train_ds.map(preprocess_image_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.shuffle(buffer_size=10).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        print(f\"TFDS DIV2K dataset processing configured with BATCH_SIZE={batch_size}.\")\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"TFDS Loading failed: {e}. Falling back to simulation.\")\n",
        "\n",
        "        print(f\"Creating SIMULATED DIV2K dataset: {num_samples} samples, batch size {batch_size}\")\n",
        "        lr_data = np.random.rand(num_samples, *lr_shape).astype(np.float32)\n",
        "        hr_data = np.random.rand(num_samples, *hr_shape).astype(np.float32)\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((lr_data, hr_data))\n",
        "        dataset = dataset.shuffle(buffer_size=100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "#Load the BSD100 Test Set\n",
        "def load_test_dataset(scale=SCALE_FACTOR):\n",
        "    \"\"\"\n",
        "    Loads the BSD100 dataset for testing, generating LR images via bicubic downsampling\n",
        "    and pairing them with the HR ground truth.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\nAttempting to load BSD100 (B100) Test Set via TFDS...\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            test_ds = tfds.load('bsd100', split='test', as_supervised=True)\n",
        "        except:\n",
        "            # Fallback for datasets without a 'test' split (e.g., Set5/14)\n",
        "            test_ds = tfds.load('bsd100', split='all', as_supervised=True)\n",
        "\n",
        "        def preprocess_test_image(hr_img, _):\n",
        "\n",
        "            hr_img = tf.image.convert_image_dtype(hr_img, tf.float32)\n",
        "\n",
        "            #Generate LR image using Bicubic downsampling (as per Unified Preprocessing standard)\n",
        "            hr_h = tf.shape(hr_img)[0]\n",
        "            hr_w = tf.shape(hr_img)[1]\n",
        "\n",
        "            # Calculate LR dimensions\n",
        "            lr_h = hr_h // scale\n",
        "            lr_w = hr_w // scale\n",
        "\n",
        "            # Downsample using Bicubic method\n",
        "            lr_img = tf.image.resize(\n",
        "                hr_img,\n",
        "                size=[lr_h, lr_w],\n",
        "                method=tf.image.ResizeMethod.BICUBIC\n",
        "            )\n",
        "\n",
        "            # Return LR input and HR ground truth\n",
        "            return lr_img, hr_img\n",
        "\n",
        "        # Apply preprocessing (downsampling and normalization)\n",
        "        test_dataset = test_ds.map(preprocess_test_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Batch size of 1 is typical for evaluation to handle variable image sizes\n",
        "        test_dataset = test_dataset.batch(1).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        print(\"BSD100 Test Set configured for evaluation (LR generated via Bicubic downsampling).\")\n",
        "        return test_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"TFDS BSD100 Loading failed: {e}. Returning None.\")\n",
        "        return None\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. KERAS MODEL DEFINITION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def ResidualBlock(x):\n",
        "    \"\"\"A standard Residual Block structure.\"\"\"\n",
        "    x_res = x\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same')(x)\n",
        "    x = tf.keras.layers.Add()([x, x_res])\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "def create_sr_model(scale=SCALE_FACTOR, lr_size=LR_SIZE):\n",
        "    \"\"\"Defines a concrete Super-Resolution model structure.\"\"\"\n",
        "    print(\"\\nDefining Keras Super-Resolution Model (Multi-Block Residual Style - 5 Blocks)...\")\n",
        "\n",
        "\n",
        "    input_tensor = tf.keras.Input(shape=(None, None, 3))\n",
        "\n",
        "    # 1. Feature Extraction (Initial Conv)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=9, padding='same', activation='relu')(input_tensor)\n",
        "    global_res = x\n",
        "\n",
        "    # 2. Residual Blocks\n",
        "    for _ in range(5):\n",
        "        x = ResidualBlock(x)\n",
        "\n",
        "    # Global Residual Skip Connection\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same')(x)\n",
        "    x = tf.keras.layers.Add()([x, global_res])\n",
        "\n",
        "    # 3. Upscaling (Sub-pixel Convolution is a better alternative but UpSampling2D is simpler here)\n",
        "    x = tf.keras.layers.UpSampling2D(size=(scale, scale), interpolation='nearest')(x)\n",
        "\n",
        "    # 4. Reconstruction (Final layer)\n",
        "    output_tensor = tf.keras.layers.Conv2D(filters=3, kernel_size=5, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor, name=\"SR_Residual_Model_MAE_Loss\")\n",
        "\n",
        "    # Compile the model using MAE Loss, the custom Adam optimizer, and quantitative metrics\n",
        "    model.compile(optimizer=optim_edsr, loss='mae', metrics=[psnr_metric, ssim_metric])\n",
        "    print(\"SR Model defined and compiled successfully using **MAE Loss**, Custom Adam Optimizer, and PSNR/SSIM Metrics.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. EVALUATION COMPONENTS\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def predict_super_resolution(model, lr_image):\n",
        "    \"\"\"\n",
        "    Uses the trained Keras model to generate a super-resolved image.\n",
        "    Accepts normalized [0, 1] tensor.\n",
        "    \"\"\"\n",
        "    # Ensure it's batched\n",
        "    lr_batched = tf.expand_dims(lr_image, axis=0)\n",
        "\n",
        "    # Model prediction (outputs float [0, 1])\n",
        "    sr_float_batched = model.predict(lr_batched, verbose=0)\n",
        "    sr_float = tf.squeeze(sr_float_batched, axis=0)\n",
        "\n",
        "    # Convert back to uint8 [0, 255] for display\n",
        "    sr_image_uint8 = tf.cast(tf.clip_by_value(sr_float * 255.0, 0, 255), tf.uint8)\n",
        "\n",
        "    return sr_image_uint8, sr_float # Return both uint8 for display and float for metrics\n",
        "\n",
        "def plot_lr_sr_hr(lr_image, sr_image, hr_image, psnr, ssim, index, scale=SCALE_FACTOR):\n",
        "    \"\"\"\n",
        "    Displays the LR Input, SR Output, and HR Ground Truth comparison by saving the plot to a file.\n",
        "    Includes calculated metrics for the current sample.\n",
        "    \"\"\"\n",
        "    # Determine the display size\n",
        "    HR_SIZE_LOCAL = hr_image.shape[0]\n",
        "\n",
        "    # Create the figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Resize LR for visualization only (using nearest neighbor for clarity)\n",
        "    lr_display = tf.image.resize(tf.cast(lr_image * 255.0, tf.uint8).numpy(), [HR_SIZE_LOCAL, HR_SIZE_LOCAL], method='nearest').numpy().astype(np.uint8)\n",
        "\n",
        "    # Convert HR and SR (uint8) for display\n",
        "    hr_display = tf.cast(hr_image * 255.0, tf.uint8).numpy()\n",
        "    sr_display = sr_image.numpy()\n",
        "\n",
        "    axes[0].imshow(lr_display)\n",
        "    axes[0].set_title(f\"Low Resolution Input (x{scale} Bicubic)\", fontsize=10)\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(sr_display.astype(np.uint8))\n",
        "    axes[1].set_title(f\"SR Output (PSNR: {psnr:.2f} dB, SSIM: {ssim:.4f})\", fontsize=10)\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(hr_display.astype(np.uint8))\n",
        "    axes[2].set_title(f\"High Resolution Ground Truth\", fontsize=10)\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "\n",
        "    plt.suptitle(f\"BSD100 Test Sample {index+1}\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot to a file instead of trying to show it interactively\n",
        "    filepath = f\"bsd100_test_comparison_sample_{index+1}.png\"\n",
        "    plt.savefig(filepath)\n",
        "    plt.close(fig) # Close the figure to free up memory\n",
        "    print(f\"Plot saved to {filepath}\")\n",
        "\n",
        "\n",
        "def run_test_evaluation_bsd100(model, test_dataset, num_samples=5):\n",
        "    \"\"\"\n",
        "    Runs evaluation on the BSD100 dataset, calculating metrics and saving plots.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting BSD100 Test Evaluation ({num_samples} Samples) ---\")\n",
        "\n",
        "    total_psnr = 0.0\n",
        "    total_ssim = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # Iterate over the first few samples for visual plotting\n",
        "    for i, (lr_batch, hr_batch) in enumerate(test_dataset.take(num_samples)):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        # Extract the single image from the batch\n",
        "        lr_img_norm = lr_batch[0] # Normalized LR [0, 1]\n",
        "        hr_img_norm = hr_batch[0] # Normalized HR [0, 1]\n",
        "\n",
        "        # Upscale the image\n",
        "        sr_img_uint8, sr_img_norm = predict_super_resolution(model, lr_img_norm)\n",
        "\n",
        "        # Calculate metrics for this sample\n",
        "        current_psnr = psnr_metric(hr_img_norm, sr_img_norm).numpy()\n",
        "        # tf.image.ssim returns a single value if both inputs have the same shape\n",
        "        current_ssim = tf.image.ssim(hr_img_norm, sr_img_norm, max_val=1.0).numpy()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_psnr += current_psnr\n",
        "        total_ssim += current_ssim\n",
        "        count += 1\n",
        "\n",
        "        # Plot the LR, SR, and HR results, including metrics\n",
        "        plot_lr_sr_hr(lr_img_norm, sr_img_uint8, hr_img_norm, current_psnr, current_ssim, i)\n",
        "\n",
        "    # Calculate and print final mean metrics if data was processed\n",
        "    if count > 0:\n",
        "        mean_psnr = total_psnr / count\n",
        "        mean_ssim = total_ssim / count\n",
        "        print(f\"\\n--- RESULTS ON BSD100 TEST SET (First {count} Samples) ---\")\n",
        "        print(f\"Mean PSNR: {mean_psnr:.4f} dB\")\n",
        "        print(f\"Mean SSIM: {mean_ssim:.4f}\")\n",
        "    else:\n",
        "        print(\"No BSD100 test samples were available for evaluation.\")\n",
        "\n",
        "    print(\"--- BSD100 Test Evaluation Complete. ---\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6. EXECUTION AND EVALUATION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. Initialize the training dataset (DIV2K)\n",
        "    train_dataset = load_or_simulate_dataset(\n",
        "        num_samples=TRAIN_SAMPLES,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        lr_shape=(LR_SIZE, LR_SIZE, 3),\n",
        "        hr_shape=(HR_SIZE, HR_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # 1.5. Initialize the test dataset (BSD100)\n",
        "    test_dataset = load_test_dataset()\n",
        "\n",
        "    # 2. Create the fully defined SR model\n",
        "    sr_model = create_sr_model()\n",
        "\n",
        "    print(\"\\n\" * 2)\n",
        "\n",
        "    # 3. Training\n",
        "    print(f\"--- Starting Training ({EPOCHS} Epochs with MAE Loss, Custom LR Schedule, and Checkpointing) ---\")\n",
        "    sr_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[save_best_cb], # Pass the checkpoint callback here\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"--- Training Complete. Starting Evaluation. ---\")\n",
        "\n",
        "    try:\n",
        "        # 4. Evaluation using BSD100\n",
        "        if test_dataset:\n",
        "            # Run the formal test evaluation on BSD100\n",
        "            run_test_evaluation_bsd100(sr_model, test_dataset, num_samples=5)\n",
        "        else:\n",
        "            # Fallback to ad-hoc visual evaluation on training data if BSD100 failed to load\n",
        "            print(\"\\nWARNING: Could not load BSD100. Running ad-hoc visual check on training data.\")\n",
        "            # We'll keep a simplified version of the old function name for this fallback\n",
        "            def run_ad_hoc_evaluation_fallback(model, dataset, num_samples=8):\n",
        "                print(f\"\\n--- Starting Ad-Hoc Visual Evaluation ({num_samples} Samples, Model: MAE Loss) ---\")\n",
        "                for i, (lr_batch, _) in enumerate(dataset.take(num_samples)):\n",
        "                    if i >= num_samples: break\n",
        "                    lowres_img = lr_batch[0]\n",
        "                    # Note: predict_super_resolution now takes normalized input\n",
        "                    sr_img_uint8, _ = predict_super_resolution(model, lowres_img)\n",
        "\n",
        "                    # Create a simple LR vs SR plot (using the old plotting function structure)\n",
        "                    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "                    # Resizing LR input to match SR output size for visualization\n",
        "                    lr_display = tf.image.resize(tf.cast(lowres_img * 255.0, tf.uint8).numpy(), [sr_img_uint8.shape[0], sr_img_uint8.shape[1]], method='nearest').numpy().astype(np.uint8)\n",
        "                    axes[0].imshow(lr_display)\n",
        "                    axes[0].set_title(f\"Low Resolution Input\", fontsize=10)\n",
        "                    axes[0].axis(\"off\")\n",
        "                    axes[1].imshow(sr_img_uint8.numpy().astype(np.uint8))\n",
        "                    axes[1].set_title(f\"Super-Resolution Output\", fontsize=10)\n",
        "                    axes[1].axis(\"off\")\n",
        "                    plt.tight_layout()\n",
        "                    filepath = f\"sr_comparison_sample_{i+1}_fallback.png\"\n",
        "                    plt.savefig(filepath); plt.close(fig)\n",
        "                    print(f\"Fallback plot saved to {filepath}\")\n",
        "                print(\"--- Ad-Hoc Visual Evaluation Complete. ---\")\n",
        "\n",
        "            run_ad_hoc_evaluation_fallback(sr_model, train_dataset, num_samples=8)\n",
        "\n",
        "\n",
        "        print(\"\\nModel Evaluation successfully completed. \")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFATAL ERROR: The script failed unexpectedly during evaluation.\")\n",
        "        print(f\"Error detail: {e}\")\n",
        "        sys.exit(1)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "--------------------------------------------------\n",
            "Attempting to load real DIV2K dataset via TFDS...\n",
            "Initial TFDS samples: 800\n",
            "TFDS DIV2K dataset processing configured with BATCH_SIZE=8.\n",
            "\n",
            "Attempting to load BSD100 (B100) Test Set via TFDS...\n",
            "TFDS BSD100 Loading failed: Dataset bsd100 not found.\n",
            "Available datasets:\n",
            "\t- abstract_reasoning\n",
            "\t- accentdb\n",
            "\t- aeslc\n",
            "\t- aflw2k3d\n",
            "\t- ag_news_subset\n",
            "\t- ai2_arc\n",
            "\t- ai2_arc_with_ir\n",
            "\t- ai2dcaption\n",
            "\t- aloha_mobile\n",
            "\t- amazon_us_reviews\n",
            "\t- anli\n",
            "\t- answer_equivalence\n",
            "\t- arc\n",
            "\t- asimov_dilemmas_auto_val\n",
            "\t- asimov_dilemmas_scifi_train\n",
            "\t- asimov_dilemmas_scifi_val\n",
            "\t- asimov_injury_val\n",
            "\t- asimov_multimodal_auto_val\n",
            "\t- asimov_multimodal_manual_val\n",
            "\t- asqa\n",
            "\t- asset\n",
            "\t- assin2\n",
            "\t- asu_table_top_converted_externally_to_rlds\n",
            "\t- austin_buds_dataset_converted_externally_to_rlds\n",
            "\t- austin_sailor_dataset_converted_externally_to_rlds\n",
            "\t- austin_sirius_dataset_converted_externally_to_rlds\n",
            "\t- bair_robot_pushing_small\n",
            "\t- bc_z\n",
            "\t- bccd\n",
            "\t- beans\n",
            "\t- bee_dataset\n",
            "\t- beir\n",
            "\t- berkeley_autolab_ur5\n",
            "\t- berkeley_cable_routing\n",
            "\t- berkeley_fanuc_manipulation\n",
            "\t- berkeley_gnm_cory_hall\n",
            "\t- berkeley_gnm_recon\n",
            "\t- berkeley_gnm_sac_son\n",
            "\t- berkeley_mvp_converted_externally_to_rlds\n",
            "\t- berkeley_rpt_converted_externally_to_rlds\n",
            "\t- big_patent\n",
            "\t- bigearthnet\n",
            "\t- billsum\n",
            "\t- binarized_mnist\n",
            "\t- binary_alpha_digits\n",
            "\t- ble_wind_field\n",
            "\t- blimp\n",
            "\t- booksum\n",
            "\t- bool_q\n",
            "\t- bot_adversarial_dialogue\n",
            "\t- bridge\n",
            "\t- bridge_data_msr\n",
            "\t- bucc\n",
            "\t- c4\n",
            "\t- c4_wsrs\n",
            "\t- caltech101\n",
            "\t- caltech_birds2010\n",
            "\t- caltech_birds2011\n",
            "\t- cardiotox\n",
            "\t- cars196\n",
            "\t- cassava\n",
            "\t- cats_vs_dogs\n",
            "\t- celeb_a\n",
            "\t- celeb_a_hq\n",
            "\t- cfq\n",
            "\t- cherry_blossoms\n",
            "\t- chexpert\n",
            "\t- cifar10\n",
            "\t- cifar100\n",
            "\t- cifar100_n\n",
            "\t- cifar10_1\n",
            "\t- cifar10_corrupted\n",
            "\t- cifar10_h\n",
            "\t- cifar10_n\n",
            "\t- citrus_leaves\n",
            "\t- cityscapes\n",
            "\t- civil_comments\n",
            "\t- clevr\n",
            "\t- clic\n",
            "\t- clinc_oos\n",
            "\t- cmaterdb\n",
            "\t- cmu_franka_exploration_dataset_converted_externally_to_rlds\n",
            "\t- cmu_play_fusion\n",
            "\t- cmu_stretch\n",
            "\t- cnn_dailymail\n",
            "\t- coco\n",
            "\t- coco_captions\n",
            "\t- coil100\n",
            "\t- colorectal_histology\n",
            "\t- colorectal_histology_large\n",
            "\t- columbia_cairlab_pusht_real\n",
            "\t- common_voice\n",
            "\t- conll2002\n",
            "\t- conll2003\n",
            "\t- conq_hose_manipulation\n",
            "\t- controlled_noisy_web_labels\n",
            "\t- coqa\n",
            "\t- corr2cause\n",
            "\t- cos_e\n",
            "\t- cosmos_qa\n",
            "\t- covid19\n",
            "\t- covid19sum\n",
            "\t- crema_d\n",
            "\t- criteo\n",
            "\t- cs_restaurants\n",
            "\t- curated_breast_imaging_ddsm\n",
            "\t- cycle_gan\n",
            "\t- d4rl_adroit_door\n",
            "\t- d4rl_adroit_hammer\n",
            "\t- d4rl_adroit_pen\n",
            "\t- d4rl_adroit_relocate\n",
            "\t- d4rl_antmaze\n",
            "\t- d4rl_mujoco_ant\n",
            "\t- d4rl_mujoco_halfcheetah\n",
            "\t- d4rl_mujoco_hopper\n",
            "\t- d4rl_mujoco_walker2d\n",
            "\t- dart\n",
            "\t- databricks_dolly\n",
            "\t- davis\n",
            "\t- deep1b\n",
            "\t- deep_weeds\n",
            "\t- definite_pronoun_resolution\n",
            "\t- dementiabank\n",
            "\t- diabetic_retinopathy_detection\n",
            "\t- diamonds\n",
            "\t- dices\n",
            "\t- div2k\n",
            "\t- div2k\n",
            "\t- dlr_edan_shared_control_converted_externally_to_rlds\n",
            "\t- dlr_sara_grid_clamp_converted_externally_to_rlds\n",
            "\t- dlr_sara_pour_converted_externally_to_rlds\n",
            "\t- dmlab\n",
            "\t- dobbe\n",
            "\t- doc_nli\n",
            "\t- dolma\n",
            "\t- dolphin_number_word\n",
            "\t- domainnet\n",
            "\t- downsampled_imagenet\n",
            "\t- drop\n",
            "\t- dsprites\n",
            "\t- dtd\n",
            "\t- duke_ultrasound\n",
            "\t- e2e_cleaned\n",
            "\t- efron_morris75\n",
            "\t- emnist\n",
            "\t- eraser_multi_rc\n",
            "\t- esnli\n",
            "\t- eth_agent_affordances\n",
            "\t- eurosat\n",
            "\t- fashion_mnist\n",
            "\t- flic\n",
            "\t- flores\n",
            "\t- fmb\n",
            "\t- food101\n",
            "\t- forest_fires\n",
            "\t- fractal20220817_data\n",
            "\t- fuss\n",
            "\t- gap\n",
            "\t- geirhos_conflict_stimuli\n",
            "\t- gem\n",
            "\t- genomics_ood\n",
            "\t- german_credit_numeric\n",
            "\t- gigaword\n",
            "\t- glove100_angular\n",
            "\t- glue\n",
            "\t- goemotions\n",
            "\t- gov_report\n",
            "\t- gpt3\n",
            "\t- gref\n",
            "\t- groove\n",
            "\t- grounded_scan\n",
            "\t- gsm8k\n",
            "\t- gtzan\n",
            "\t- gtzan_music_speech\n",
            "\t- hellaswag\n",
            "\t- higgs\n",
            "\t- hillstrom\n",
            "\t- horses_or_humans\n",
            "\t- howell\n",
            "\t- i_naturalist2017\n",
            "\t- i_naturalist2018\n",
            "\t- i_naturalist2021\n",
            "\t- iamlab_cmu_pickup_insert_converted_externally_to_rlds\n",
            "\t- imagenet2012\n",
            "\t- imagenet2012_corrupted\n",
            "\t- imagenet2012_fewshot\n",
            "\t- imagenet2012_multilabel\n",
            "\t- imagenet2012_real\n",
            "\t- imagenet2012_subset\n",
            "\t- imagenet_a\n",
            "\t- imagenet_lt\n",
            "\t- imagenet_pi\n",
            "\t- imagenet_r\n",
            "\t- imagenet_resized\n",
            "\t- imagenet_sketch\n",
            "\t- imagenet_v2\n",
            "\t- imagenette\n",
            "\t- imagewang\n",
            "\t- imdb_reviews\n",
            "\t- imperialcollege_sawyer_wrist_cam\n",
            "\t- io_ai_tech\n",
            "\t- irc_disentanglement\n",
            "\t- iris\n",
            "\t- istella\n",
            "\t- jaco_play\n",
            "\t- kaist_nonprehensile_converted_externally_to_rlds\n",
            "\t- kddcup99\n",
            "\t- kitti\n",
            "\t- kmnist\n",
            "\t- kuka\n",
            "\t- laion400m\n",
            "\t- lambada\n",
            "\t- lbpp\n",
            "\t- lfw\n",
            "\t- librispeech\n",
            "\t- librispeech_lm\n",
            "\t- libritts\n",
            "\t- ljspeech\n",
            "\t- lm1b\n",
            "\t- locomotion\n",
            "\t- lost_and_found\n",
            "\t- lsun\n",
            "\t- lvis\n",
            "\t- malaria\n",
            "\t- maniskill_dataset_converted_externally_to_rlds\n",
            "\t- math_dataset\n",
            "\t- math_qa\n",
            "\t- mctaco\n",
            "\t- media_sum\n",
            "\t- mimic_play\n",
            "\t- mlqa\n",
            "\t- mnist\n",
            "\t- mnist_corrupted\n",
            "\t- movie_lens\n",
            "\t- movie_rationales\n",
            "\t- movielens\n",
            "\t- moving_mnist\n",
            "\t- mrqa\n",
            "\t- mslr_web\n",
            "\t- mt_opt\n",
            "\t- mtnt\n",
            "\t- multi_news\n",
            "\t- multi_nli\n",
            "\t- multi_nli_mismatch\n",
            "\t- natural_instructions\n",
            "\t- natural_questions\n",
            "\t- natural_questions_open\n",
            "\t- newsroom\n",
            "\t- nsynth\n",
            "\t- nyu_depth_v2\n",
            "\t- nyu_door_opening_surprising_effectiveness\n",
            "\t- nyu_franka_play_dataset_converted_externally_to_rlds\n",
            "\t- nyu_rot_dataset_converted_externally_to_rlds\n",
            "\t- ogbg_molpcba\n",
            "\t- omniglot\n",
            "\t- open_images_challenge2019_detection\n",
            "\t- open_images_v4\n",
            "\t- openbookqa\n",
            "\t- opinion_abstracts\n",
            "\t- opinosis\n",
            "\t- opus\n",
            "\t- oxford_flowers102\n",
            "\t- oxford_iiit_pet\n",
            "\t- para_crawl\n",
            "\t- pass\n",
            "\t- patch_camelyon\n",
            "\t- paws_wiki\n",
            "\t- paws_x_wiki\n",
            "\t- penguins\n",
            "\t- pet_finder\n",
            "\t- pg19\n",
            "\t- piqa\n",
            "\t- places365_small\n",
            "\t- placesfull\n",
            "\t- plant_leaves\n",
            "\t- plant_village\n",
            "\t- plantae_k\n",
            "\t- plex_robosuite\n",
            "\t- pneumonia_mnist\n",
            "\t- protein_net\n",
            "\t- q_re_cc\n",
            "\t- qa4mre\n",
            "\t- qasc\n",
            "\t- qm9\n",
            "\t- quac\n",
            "\t- quality\n",
            "\t- quickdraw_bitmap\n",
            "\t- race\n",
            "\t- radon\n",
            "\t- real_toxicity_prompts\n",
            "\t- reddit\n",
            "\t- reddit_disentanglement\n",
            "\t- reddit_tifu\n",
            "\t- ref_coco\n",
            "\t- resisc45\n",
            "\t- rlu_atari\n",
            "\t- rlu_atari_checkpoints\n",
            "\t- rlu_atari_checkpoints_ordered\n",
            "\t- rlu_control_suite\n",
            "\t- rlu_dmlab_explore_object_rewards_few\n",
            "\t- rlu_dmlab_explore_object_rewards_many\n",
            "\t- rlu_dmlab_rooms_select_nonmatching_object\n",
            "\t- rlu_dmlab_rooms_watermaze\n",
            "\t- rlu_dmlab_seekavoid_arena01\n",
            "\t- rlu_locomotion\n",
            "\t- rlu_rwrl\n",
            "\t- robo_set\n",
            "\t- robomimic_mg\n",
            "\t- robomimic_mh\n",
            "\t- robomimic_ph\n",
            "\t- robonet\n",
            "\t- robosuite_panda_pick_place_can\n",
            "\t- roboturk\n",
            "\t- rock_paper_scissors\n",
            "\t- rock_you\n",
            "\t- s3o4d\n",
            "\t- salient_span_wikipedia\n",
            "\t- samsum\n",
            "\t- savee\n",
            "\t- scan\n",
            "\t- scene_parse150\n",
            "\t- schema_guided_dialogue\n",
            "\t- sci_tail\n",
            "\t- scicite\n",
            "\t- scientific_papers\n",
            "\t- scrolls\n",
            "\t- segment_anything\n",
            "\t- sentiment140\n",
            "\t- shapes3d\n",
            "\t- sift1m\n",
            "\t- simpte\n",
            "\t- siscore\n",
            "\t- smallnorb\n",
            "\t- smart_buildings\n",
            "\t- smartwatch_gestures\n",
            "\t- snli\n",
            "\t- so2sat\n",
            "\t- speech_commands\n",
            "\t- spoc_robot\n",
            "\t- spoken_digit\n",
            "\t- squad\n",
            "\t- squad_question_generation\n",
            "\t- stanford_dogs\n",
            "\t- stanford_hydra_dataset_converted_externally_to_rlds\n",
            "\t- stanford_kuka_multimodal_dataset_converted_externally_to_rlds\n",
            "\t- stanford_mask_vit_converted_externally_to_rlds\n",
            "\t- stanford_online_products\n",
            "\t- stanford_robocook_converted_externally_to_rlds\n",
            "\t- star_cfq\n",
            "\t- starcraft_video\n",
            "\t- stl10\n",
            "\t- story_cloze\n",
            "\t- summscreen\n",
            "\t- sun397\n",
            "\t- super_glue\n",
            "\t- svhn_cropped\n",
            "\t- symmetric_solids\n",
            "\t- taco_play\n",
            "\t- tao\n",
            "\t- tatoeba\n",
            "\t- ted_hrlr_translate\n",
            "\t- ted_multi_translate\n",
            "\t- tedlium\n",
            "\t- tf_flowers\n",
            "\t- the300w_lp\n",
            "\t- tidybot\n",
            "\t- tiny_shakespeare\n",
            "\t- titanic\n",
            "\t- tokyo_u_lsmo_converted_externally_to_rlds\n",
            "\t- toto\n",
            "\t- trec\n",
            "\t- trivia_qa\n",
            "\t- tydi_qa\n",
            "\t- uc_merced\n",
            "\t- ucf101\n",
            "\t- ucsd_kitchen_dataset_converted_externally_to_rlds\n",
            "\t- ucsd_pick_and_place_dataset_converted_externally_to_rlds\n",
            "\t- uiuc_d3field\n",
            "\t- unified_qa\n",
            "\t- universal_dependencies\n",
            "\t- unnatural_instructions\n",
            "\t- usc_cloth_sim_converted_externally_to_rlds\n",
            "\t- user_libri_audio\n",
            "\t- user_libri_text\n",
            "\t- utaustin_mutex\n",
            "\t- utokyo_pr2_opening_fridge_converted_externally_to_rlds\n",
            "\t- utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds\n",
            "\t- utokyo_saytap_converted_externally_to_rlds\n",
            "\t- utokyo_xarm_bimanual_converted_externally_to_rlds\n",
            "\t- utokyo_xarm_pick_and_place_converted_externally_to_rlds\n",
            "\t- vctk\n",
            "\t- vima_converted_externally_to_rlds\n",
            "\t- viola\n",
            "\t- visual_domain_decathlon\n",
            "\t- voc\n",
            "\t- voxceleb\n",
            "\t- voxforge\n",
            "\t- wake_vision\n",
            "\t- waymo_open_dataset\n",
            "\t- web_graph\n",
            "\t- web_nlg\n",
            "\t- web_questions\n",
            "\t- webvid\n",
            "\t- wider_face\n",
            "\t- wiki40b\n",
            "\t- wiki_auto\n",
            "\t- wiki_bio\n",
            "\t- wiki_dialog\n",
            "\t- wiki_table_questions\n",
            "\t- wiki_table_text\n",
            "\t- wikiann\n",
            "\t- wikihow\n",
            "\t- wikipedia\n",
            "\t- wikipedia_toxicity_subtypes\n",
            "\t- wine_quality\n",
            "\t- winogrande\n",
            "\t- wit\n",
            "\t- wit_kaggle\n",
            "\t- wmt13_translate\n",
            "\t- wmt14_translate\n",
            "\t- wmt15_translate\n",
            "\t- wmt16_translate\n",
            "\t- wmt17_translate\n",
            "\t- wmt18_translate\n",
            "\t- wmt19_translate\n",
            "\t- wmt_t2t_translate\n",
            "\t- wmt_translate\n",
            "\t- wordnet\n",
            "\t- wsc273\n",
            "\t- xnli\n",
            "\t- xquad\n",
            "\t- xsum\n",
            "\t- xtreme_pawsx\n",
            "\t- xtreme_pos\n",
            "\t- xtreme_s\n",
            "\t- xtreme_xnli\n",
            "\t- yahoo_ltrc\n",
            "\t- yelp_polarity_reviews\n",
            "\t- yes_no\n",
            "\t- youtube_vis\n",
            "\n",
            "Check that:\n",
            "    - if dataset was added recently, it may only be available\n",
            "      in `tfds-nightly`\n",
            "    - the dataset name is spelled correctly\n",
            "    - dataset class defines all base class abstract methods\n",
            "    - the module defining the dataset class is imported\n",
            "\n",
            "The builder directory /root/tensorflow_datasets/bsd100 doesn't contain any versions.\n",
            "No builder could be found in the directory: /root/tensorflow_datasets for the builder: bsd100.\n",
            "No registered data_dirs were found in:\n",
            "\t- /root/tensorflow_datasets\n",
            ". Returning None.\n",
            "\n",
            "Defining Keras Super-Resolution Model (Multi-Block Residual Style - 5 Blocks)...\n",
            "SR Model defined and compiled successfully using **MAE Loss**, Custom Adam Optimizer, and PSNR/SSIM Metrics.\n",
            "\n",
            "\n",
            "\n",
            "--- Starting Training (30 Epochs with MAE Loss, Custom LR Schedule, and Checkpointing) ---\n",
            "Epoch 1/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - loss: 0.2341 - psnr_metric: 11.4342 - ssim_metric: 0.2799\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - loss: 0.2339 - psnr_metric: 11.5255 - ssim_metric: 0.2827\n",
            "Epoch 3/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - loss: 0.2341 - psnr_metric: 11.4948 - ssim_metric: 0.2740\n",
            "Epoch 4/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2288 - psnr_metric: 11.6845 - ssim_metric: 0.2808\n",
            "Epoch 5/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - loss: 0.2329 - psnr_metric: 11.5414 - ssim_metric: 0.2800\n",
            "Epoch 6/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - loss: 0.2290 - psnr_metric: 11.6957 - ssim_metric: 0.2823\n",
            "Epoch 7/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - loss: 0.2305 - psnr_metric: 11.6256 - ssim_metric: 0.2815\n",
            "Epoch 8/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2278 - psnr_metric: 11.7359 - ssim_metric: 0.2805\n",
            "Epoch 9/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 0.2305 - psnr_metric: 11.6147 - ssim_metric: 0.2836\n",
            "Epoch 10/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - loss: 0.2292 - psnr_metric: 11.6793 - ssim_metric: 0.2793\n",
            "Epoch 11/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2290 - psnr_metric: 11.6890 - ssim_metric: 0.2796\n",
            "Epoch 12/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - loss: 0.2273 - psnr_metric: 11.7269 - ssim_metric: 0.2868\n",
            "Epoch 13/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2308 - psnr_metric: 11.5941 - ssim_metric: 0.2810\n",
            "Epoch 14/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2270 - psnr_metric: 11.7366 - ssim_metric: 0.2795\n",
            "Epoch 15/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2288 - psnr_metric: 11.6866 - ssim_metric: 0.2823\n",
            "Epoch 16/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - loss: 0.2267 - psnr_metric: 11.7481 - ssim_metric: 0.2797\n",
            "Epoch 17/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - loss: 0.2246 - psnr_metric: 11.8303 - ssim_metric: 0.2835\n",
            "Epoch 18/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2268 - psnr_metric: 11.6943 - ssim_metric: 0.2838\n",
            "Epoch 19/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - loss: 0.2238 - psnr_metric: 11.8508 - ssim_metric: 0.2798\n",
            "Epoch 20/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2231 - psnr_metric: 11.8475 - ssim_metric: 0.2898\n",
            "Epoch 21/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 0.2275 - psnr_metric: 11.7261 - ssim_metric: 0.2829\n",
            "Epoch 22/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2229 - psnr_metric: 11.8360 - ssim_metric: 0.2821\n",
            "Epoch 23/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - loss: 0.2222 - psnr_metric: 11.8249 - ssim_metric: 0.2818\n",
            "Epoch 24/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2230 - psnr_metric: 11.8279 - ssim_metric: 0.2746\n",
            "Epoch 25/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - loss: 0.2247 - psnr_metric: 11.7512 - ssim_metric: 0.2838\n",
            "Epoch 26/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2250 - psnr_metric: 11.7593 - ssim_metric: 0.2776\n",
            "Epoch 27/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2237 - psnr_metric: 11.7802 - ssim_metric: 0.2760\n",
            "Epoch 28/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2217 - psnr_metric: 11.8555 - ssim_metric: 0.2817\n",
            "Epoch 29/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2222 - psnr_metric: 11.8374 - ssim_metric: 0.2873\n",
            "Epoch 30/30\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.2196 - psnr_metric: 11.9244 - ssim_metric: 0.2814\n",
            "--- Training Complete. Starting Evaluation. ---\n",
            "\n",
            "WARNING: Could not load BSD100. Running ad-hoc visual check on training data.\n",
            "\n",
            "--- Starting Ad-Hoc Visual Evaluation (8 Samples, Model: MAE Loss) ---\n",
            "Fallback plot saved to sr_comparison_sample_1_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_2_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_3_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_4_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_5_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_6_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_7_fallback.png\n",
            "Fallback plot saved to sr_comparison_sample_8_fallback.png\n",
            "--- Ad-Hoc Visual Evaluation Complete. ---\n",
            "\n",
            "Model Evaluation successfully completed. \n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tZbXA8_HIRs",
        "outputId": "4871313b-f271-4d34-fd84-8c13f4efbc0d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}